# Enterprise RAG Pipeline Configuration

# Vector Database Configuration
vector_db:
  provider: "qdrant"  # Options: qdrant, weaviate, chromadb
  host: "localhost"
  port: 6333
  collection_name: "enterprise_documents"
  embedding_dimension: 768

# Embedding Model Configuration
embeddings:
  model: "sentence-transformers/all-mpnet-base-v2"
  device: "cuda"  # Options: cuda, cpu, mps
  batch_size: 32
  normalize_embeddings: true

# Retrieval Configuration
retrieval:
  # Hybrid Search Weights
  vector_weight: 0.7  # 70% vector search
  bm25_weight: 0.3    # 30% BM25 search
  
  # Retrieval Parameters
  top_k: 20           # Initial retrieval count
  rerank: true        # Enable cross-encoder reranking
  rerank_top_k: 20    # Documents to rerank
  final_top_k: 5      # Final documents to return
  
  # Metadata Filtering
  filters:
    enabled: true
    # Example filters:
    # department: "HR"
    # classification: ["public", "internal"]
    # date_range: 
    #   start: "2024-01-01"
    #   end: "2024-12-31"

# Reranking Configuration
reranking:
  enabled: true
  model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  device: "cuda"
  batch_size: 16

# LLM Configuration
llm:
  provider: "openai"  # Options: openai, anthropic, vllm, ollama
  model: "gpt-4"
  temperature: 0.1
  max_tokens: 1000
  top_p: 0.95
  
  # Alternative: Local model with vLLM
  # provider: "vllm"
  # model: "meta-llama/Llama-2-7b-chat-hf"
  # base_url: "http://localhost:8000"

# Prompt Configuration
prompts:
  system_prompt: |
    You are a helpful AI assistant that answers questions based on enterprise documents.
    Always cite your sources and be factual. If you don't know the answer, say so.
  
  query_template: |
    Context:
    {context}
    
    Question: {query}
    
    Answer the question based only on the provided context. Include relevant citations.

# Caching Configuration
caching:
  enabled: true
  provider: "redis"
  host: "localhost"
  port: 6379
  ttl: 3600  # Cache TTL in seconds (1 hour)
  
  # Cache levels
  cache_embeddings: true
  cache_responses: true

# Monitoring and Logging
monitoring:
  enabled: true
  log_level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR
  
  # Metrics
  prometheus_enabled: true
  prometheus_port: 9090
  
  # Tracing
  tracing_enabled: true
  
  # Logging
  structured_logging: true
  log_format: "json"

# Security Configuration
security:
  # Authentication
  auth_enabled: true
  jwt_secret: "${JWT_SECRET}"  # Use environment variable
  
  # Rate Limiting
  rate_limit_enabled: true
  rate_limit_per_minute: 60
  
  # PII Detection
  pii_detection_enabled: true
  pii_redaction_enabled: true

# Data Processing Configuration
document_processing:
  # Supported file types
  supported_formats: ["pdf", "docx", "txt", "xlsx", "pptx"]
  
  # Chunking Strategy
  chunking:
    strategy: "hierarchical"  # Options: hierarchical, semantic, fixed
    chunk_size: 512
    chunk_overlap: 50
    respect_sections: true
  
  # Quality Detection
  quality_detection:
    enabled: true
    min_quality_score: 0.7
    ocr_enabled: true

# Performance Configuration
performance:
  # Async Processing
  async_enabled: true
  max_concurrent_requests: 10
  
  # Batch Processing
  batch_size: 32
  
  # Timeout
  request_timeout: 30  # seconds

# Environment-specific Overrides
environments:
  development:
    llm:
      model: "gpt-3.5-turbo"
    caching:
      ttl: 300  # 5 minutes
  
  production:
    llm:
      model: "gpt-4"
    caching:
      ttl: 3600  # 1 hour
    monitoring:
      log_level: "WARNING"
